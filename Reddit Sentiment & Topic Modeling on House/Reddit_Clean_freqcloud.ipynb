{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>selftext</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>realize pretty vague honestly mind long simply...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Trying to source house track I heard on dubpla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>know long shot looking song month luck hoping ...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Does anyone have a copy of Rene Breitbarth - \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>post dedicated finding unknown unidentifiable ...</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>Welcome to our weekly post for song/track IDs....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>kind remix like best d</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Looking for a really good remix of \"Foreigner ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>like minimal micro house playlist you perfect ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Minimal House / Deep Tech Playlist (Submission...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>4861</td>\n",
       "      <td>76648</td>\n",
       "      <td>looking house track long drum break conga djem...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Looking for tracks with percussive drum breaks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4862</th>\n",
       "      <td>4862</td>\n",
       "      <td>76784</td>\n",
       "      <td>trying get year contacted label said would sho...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Need Some Help Finding a Rare Tiger &amp;amp; Wood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4863</th>\n",
       "      <td>4863</td>\n",
       "      <td>76797</td>\n",
       "      <td>time year yearly round best music year rule si...</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>r/House's 2016 Favourites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4864</th>\n",
       "      <td>4864</td>\n",
       "      <td>76800</td>\n",
       "      <td>mine in particular order hard omar s heard che...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>Your favourite tracks of the year 2016?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>4865</td>\n",
       "      <td>76851</td>\n",
       "      <td>hey guy first post reddit please bare making m...</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>Is jackmaster super overrated ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4866 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  index                                           selftext  \\\n",
       "0              0     11  realize pretty vague honestly mind long simply...   \n",
       "1              1     21  know long shot looking song month luck hoping ...   \n",
       "2              2     49  post dedicated finding unknown unidentifiable ...   \n",
       "3              3     91                             kind remix like best d   \n",
       "4              4    111  like minimal micro house playlist you perfect ...   \n",
       "...          ...    ...                                                ...   \n",
       "4861        4861  76648  looking house track long drum break conga djem...   \n",
       "4862        4862  76784  trying get year contacted label said would sho...   \n",
       "4863        4863  76797  time year yearly round best music year rule si...   \n",
       "4864        4864  76800  mine in particular order hard omar s heard che...   \n",
       "4865        4865  76851  hey guy first post reddit please bare making m...   \n",
       "\n",
       "      num_comments  score                                              title  \n",
       "0                0      1  Trying to source house track I heard on dubpla...  \n",
       "1                5      3  Does anyone have a copy of Rene Breitbarth - \"...  \n",
       "2                9      4  Welcome to our weekly post for song/track IDs....  \n",
       "3                2      1  Looking for a really good remix of \"Foreigner ...  \n",
       "4                0      3  Minimal House / Deep Tech Playlist (Submission...  \n",
       "...            ...    ...                                                ...  \n",
       "4861             0      1     Looking for tracks with percussive drum breaks  \n",
       "4862             4      1  Need Some Help Finding a Rare Tiger &amp; Wood...  \n",
       "4863            13     17                          r/House's 2016 Favourites  \n",
       "4864             3     11            Your favourite tracks of the year 2016?  \n",
       "4865            16      2                    Is jackmaster super overrated ?  \n",
       "\n",
       "[4866 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('cleaned_house.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trying to source house track I heard on dubpla...</td>\n",
       "      <td>realize pretty vague honestly mind long simply...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does anyone have a copy of Rene Breitbarth - \"...</td>\n",
       "      <td>know long shot looking song month luck hoping ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Welcome to our weekly post for song/track IDs....</td>\n",
       "      <td>post dedicated finding unknown unidentifiable ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Looking for a really good remix of \"Foreigner ...</td>\n",
       "      <td>kind remix like best d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Minimal House / Deep Tech Playlist (Submission...</td>\n",
       "      <td>like minimal micro house playlist you perfect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>Looking for tracks with percussive drum breaks</td>\n",
       "      <td>looking house track long drum break conga djem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4862</th>\n",
       "      <td>Need Some Help Finding a Rare Tiger &amp;amp; Wood...</td>\n",
       "      <td>trying get year contacted label said would sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4863</th>\n",
       "      <td>r/House's 2016 Favourites</td>\n",
       "      <td>time year yearly round best music year rule si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4864</th>\n",
       "      <td>Your favourite tracks of the year 2016?</td>\n",
       "      <td>mine in particular order hard omar s heard che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>Is jackmaster super overrated ?</td>\n",
       "      <td>hey guy first post reddit please bare making m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     Trying to source house track I heard on dubpla...   \n",
       "1     Does anyone have a copy of Rene Breitbarth - \"...   \n",
       "2     Welcome to our weekly post for song/track IDs....   \n",
       "3     Looking for a really good remix of \"Foreigner ...   \n",
       "4     Minimal House / Deep Tech Playlist (Submission...   \n",
       "...                                                 ...   \n",
       "4861     Looking for tracks with percussive drum breaks   \n",
       "4862  Need Some Help Finding a Rare Tiger &amp; Wood...   \n",
       "4863                          r/House's 2016 Favourites   \n",
       "4864            Your favourite tracks of the year 2016?   \n",
       "4865                    Is jackmaster super overrated ?   \n",
       "\n",
       "                                               selftext  \n",
       "0     realize pretty vague honestly mind long simply...  \n",
       "1     know long shot looking song month luck hoping ...  \n",
       "2     post dedicated finding unknown unidentifiable ...  \n",
       "3                                kind remix like best d  \n",
       "4     like minimal micro house playlist you perfect ...  \n",
       "...                                                 ...  \n",
       "4861  looking house track long drum break conga djem...  \n",
       "4862  trying get year contacted label said would sho...  \n",
       "4863  time year yearly round best music year rule si...  \n",
       "4864  mine in particular order hard omar s heard che...  \n",
       "4865  hey guy first post reddit please bare making m...  \n",
       "\n",
       "[4866 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.dropna(subset=['title'])[['title','selftext']]\n",
    "df2 #leave it to anaylze later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicated values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4287"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take interesting columns\n",
    "df1=df[['selftext','num_comments','score','title']]\n",
    "#drop those with text=NA\n",
    "df1=df1.dropna(subset=['selftext'])\n",
    "#drop meaningless text\n",
    "df1=df1[(df1.selftext!='[removed]') & (df1.selftext!='[deleted]')].reset_index()\n",
    "#drop duplicate\n",
    "duplicate = df1.duplicated()\n",
    "print('There are', duplicate.sum(), 'duplicated values.')\n",
    "df1=df1.drop_duplicates()\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markov=df1['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       realize pretty vague honestly mind long simply...\n",
       "1       know long shot looking song month luck hoping ...\n",
       "2       post dedicated finding unknown unidentifiable ...\n",
       "3                                  kind remix like best d\n",
       "4       like minimal micro house playlist you perfect ...\n",
       "                              ...                        \n",
       "4282    looking house track long drum break conga djem...\n",
       "4283    trying get year contacted label said would sho...\n",
       "4284    time year yearly round best music year rule si...\n",
       "4285    mine in particular order hard omar s heard che...\n",
       "4286    hey guy first post reddit please bare making m...\n",
       "Name: selftext, Length: 4287, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']=df1['selftext'].str.lower()\n",
    "df1['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realize pretty vague honestly mind long simply need track house tune heard urban boogie station dubplate fm undead disco set help locating naming track appreciated'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']=df1['selftext'].apply(lambda x: remove_emoji(x))\n",
    "df1['selftext'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove url, punctuation, and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using regular expression\n",
    "import re\n",
    "def remove_other(x):\n",
    "    x = re.sub(\"\\$\",\" \", x) #remove $\n",
    "    x = re.sub(\"https*\\S+\", \" \", x) #remove url\n",
    "    x = re.sub(\"\\'\\w+\", '', x) #remove i'm,we're,let's after the '\n",
    "    x = re.sub(\"[0-9]+\", '', x) #remove numbers\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realize pretty vague honestly mind long simply need track house tune heard urban boogie station dubplate fm undead disco set help locating naming track appreciated'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']=df1['selftext'].apply(lambda x: remove_other(x))\n",
    "df1['selftext'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'further', 'so', 'against', 'd', 'into', 'of', 'to', 'some', 'that', 'we', 'once', \"shan't\", 'than', 'but', 'no', 'can', \"weren't\", 'during', 't', 'wasn', 'do', 've', 'when', 'very', 'where', \"isn't\", 'there', 'what', 'his', \"wouldn't\", 'is', 'ourselves', 'aren', 'was', 'who', 'her', 'up', 'on', 'a', 'for', 'from', 'down', 'won', 'how', 'hasn', 'did', 'shouldn', 'themselves', 'me', 'has', 'herself', 'isn', 'doing', \"don't\", 'didn', \"hadn't\", \"you've\", 'just', 'more', 'yourselves', 'those', 'theirs', 'had', 'you', 'in', 'such', \"that'll\", 'if', 'below', \"it's\", 'out', 'himself', 'above', \"you're\", \"hasn't\", 'she', 'whom', 'o', \"aren't\", 'these', 'ma', 'having', 'being', 'other', 'he', 'while', 'yourself', 'both', 'them', 'or', 'itself', \"haven't\", 'at', 'are', 'until', 'hers', 'off', 'll', 'm', 'their', 'wouldn', 'this', 'between', \"mightn't\", 'any', 'y', 'myself', 'should', 'shan', 'which', 'yours', 'your', 'mustn', 'mightn', 'why', 'will', 'after', \"should've\", \"wasn't\", 'not', 'few', 'weren', 'does', \"won't\", 'ours', 'needn', 'because', 'about', \"shouldn't\", 'be', 'most', 'all', 'again', \"mustn't\", 'haven', 'as', 'it', \"needn't\", 'ain', \"she's\", \"didn't\", 'have', 'through', 'am', 'the', 'then', 'same', 'nor', 'hadn', 'each', 'by', 's', 'over', \"you'll\", 'with', 'been', 'own', 'too', 'its', 'here', 'they', 'an', 'couldn', 'him', 'were', 'and', 'don', \"doesn't\", \"couldn't\", \"you'd\", 'i', 'before', 'under', 'my', 'only', 'now', 'doesn', 're', 'our'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\") #uncomment it when run it for the first time\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words) #all preloaded stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'realize pretty vague honestly mind long simply need track house tune heard urban boogie station dubplate fm undead disco set help locating naming track appreciated'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']=df1['selftext'].apply(lambda x: remove_stopwords(x))\n",
    "df1['selftext'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       realize pretty vague honestly mind long simply...\n",
       "1       know long shot looking song month luck hoping ...\n",
       "2       post dedicated finding unknown unidentifiable ...\n",
       "3                                    kind remix like best\n",
       "4       like minimal micro house playlist perfect road...\n",
       "                              ...                        \n",
       "4282    looking house track long drum break conga djem...\n",
       "4283    trying get year contacted label said would sho...\n",
       "4284    time year yearly round best music year rule si...\n",
       "4285    mine particular order hard omar heard chew sin...\n",
       "4286    hey guy first post reddit please bare making m...\n",
       "Name: selftext, Length: 4287, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_chars = [\"!\",'“','\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
    "              \"`\",\"{\",\"|\",\"}\",\"~\",\"–\"]\n",
    "for char in spec_chars:\n",
    "    df1['selftext'] = df1['selftext'].str.replace(char, ' ')\n",
    "    df1['selftext'] = df1['selftext'].str.split().str.join(\" \")\n",
    "df1['selftext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the resulting text to see whatelse we need to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like minimal micro house playlist perfect road trip focusing relaxing living best life sure save follow like vibe amp xb'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Lemmatization(change rules to rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like ---> like\n",
      "minimal ---> minimal\n",
      "micro ---> micro\n",
      "house ---> house\n",
      "playlist ---> playlist\n",
      "perfect ---> perfect\n",
      "road ---> road\n",
      "trip ---> trip\n",
      "focusing ---> focusing\n",
      "relaxing ---> relaxing\n",
      "living ---> living\n",
      "best ---> best\n",
      "life ---> life\n",
      "sure ---> sure\n",
      "save ---> save\n",
      "follow ---> follow\n",
      "like ---> like\n",
      "vibe ---> vibe\n",
      "amp ---> amp\n",
      "xb ---> xb\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet') #uncommented for 1st time running\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "words=df1['selftext'][4].split()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(word + \" ---> \" + lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nwords=[]\n",
    "    for word in words:\n",
    "        word=lemmatizer.lemmatize(word)\n",
    "        nwords.append(word)\n",
    "    return ' '.join(nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like minimal micro house playlist perfect road trip focusing relaxing living best life sure save follow like vibe amp xb'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['selftext']= df1['selftext'].str.split().apply(lambda x: lemmatize(x))\n",
    "df1['selftext'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 435. MiB for an array with shape (13307, 4287) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5144b6d149ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcount_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbig_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mbig_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    500\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             new_data = concatenate_block_managers(\n\u001b[0m\u001b[0;32m    503\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 435. MiB for an array with shape (13307, 4287) and data type int64"
     ]
    }
   ],
   "source": [
    "vec=CountVectorizer()\n",
    "counts=vec.fit_transform(df1['selftext'])\n",
    "counts=counts.toarray()\n",
    "count_df=pd.DataFrame(counts,columns=vec.get_feature_names())\n",
    "big_df=pd.concat((df1,count_df),axis=1)\n",
    "big_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all words across rows\n",
    "import itertools\n",
    "import collections\n",
    "def count_words(text):\n",
    "    all_words = list(itertools.chain(*text.str.split()))\n",
    "    counts = collections.Counter(all_words)\n",
    "    counts_df = pd.DataFrame(counts.most_common(100),\n",
    "                            columns=['words', 'count'])\n",
    "\n",
    "    return counts_df\n",
    "count_words(df1['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.update({'would','k','im','could','also','amp','much','one','like','get','since'})\n",
    "df1['selftext']=df1['selftext'].apply(lambda x: remove_stopwords(x))\n",
    "count_df=count_words(df1['selftext'])\n",
    "count_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(count_df.head(10), x='words', y='count', title=\"Top 10 words in Financial Independence\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list of word\n",
    "text=' '.join(count_df.words)\n",
    "\n",
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=1500, height=1000, margin=0,background_color='white').generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two grams Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "def count_2gram(text):\n",
    "    all_words = list(itertools.chain(*text.str.split()))\n",
    "    esBigrams = ngrams(all_words, 2)\n",
    "    counts = collections.Counter(esBigrams)\n",
    "    counts_df = pd.DataFrame(counts.most_common(100),\n",
    "                            columns=['words', 'count'])\n",
    "    dictionary2 = [' '.join(tup) for tup in counts_df.words]\n",
    "    counts_df.words=dictionary2\n",
    "\n",
    "    return counts_df\n",
    "counts_df2=count_2gram(df1['selftext'])\n",
    "counts_df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(counts_df2.head(10), x='words', y='count', title=\"Top 10 2-grams in Financial Independence\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = dict(counts_df2.values)\n",
    "\n",
    "wordCloud = WordCloud(max_words=200, height=1000, width=1500)\n",
    "wordCloud.generate_from_frequencies(words_dict)\n",
    "plt.title('Most frequently occurring bigrams connected by same colour and font size')\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Markov Chain to generate synthetic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def markov_text(s, n, seed,length):\n",
    "    '''\n",
    "    Generates synthetic text according to an n-th order Markov model\n",
    "    Parameter s: input string of real text\n",
    "    Parameter n: integer n, the order of the model\n",
    "    Parameter seed: the initial string that gets the Markov model started.\n",
    "    Parameter length: integer, the size of the text to generate. default=100\n",
    "    '''\n",
    "    counts = count_ngrams(s, n+1)\n",
    "    fake=seed\n",
    "    for i in range(length):\n",
    "        previous = fake[(-n):]\n",
    "        # filter dict to keep only matching grams\n",
    "        sub = {}\n",
    "        for key in counts:\n",
    "            if key[:-1] == previous: \n",
    "                sub[key] = counts[key]\n",
    "    # convert to lists for use with random.choices\n",
    "        choices = list(sub.keys())\n",
    "        weights = [sub[key] for key in choices]\n",
    "        new_gram = random.choices(choices, weights)[0]\n",
    "        new_char = new_gram[-1]\n",
    "        fake+=new_char\n",
    "    return fake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(s,n=1):\n",
    "    '''\n",
    "    Counts the number of times each n-gram occurs in a string 's'\n",
    "    Parameter s: input string\n",
    "    Parameter n: input integer n to specify 'n-gram' that need to be count\n",
    "    '''\n",
    "    D={}\n",
    "    #loop through s, untill there are not enough characters left forming n-gram\n",
    "    for i in range(len(s)-(n-1)): \n",
    "        D[s[i:i+n]]=0 # initialize values to avoid key errors\n",
    "    for i in range(len(s)-(n-1)):\n",
    "        D[s[i:i+n]]+=1 #the value of each n-gram is their counts in s\n",
    "    return D\n",
    "    \n",
    "#test the output\n",
    "count_ngrams(\"tortoise\", n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "all_words = list(itertools.chain(*Markov.str.split()))\n",
    "s=' '.join(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_text(s, n = 8, length = 400, seed = \"roth ira\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
